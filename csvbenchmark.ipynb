{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import pickle\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow as pa\n",
    "import _pickle as cPickle\n",
    "import feather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the csv for current processing\n",
    "\n",
    "csvfile = 'hydraulic_systems_demo_data.csv'\n",
    "df = pd.read_csv(csvfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time to read csv from pandas\n",
    "\n",
    "def calculate_read_csv_time(csvfile):\n",
    "    start = time.time()\n",
    "    df = pd.read_csv(csvfile)\n",
    "    csvreadtime = time.time()-start\n",
    "    return csvreadtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving in the sqlite db\n",
    "\n",
    "sqlfile = 'hydraulic.sqlite3'\n",
    "\n",
    "def calculate_df_to_sql_save_time(csvfile,sqlfile):\n",
    "    sqlconnection = sqlite3.connect(sqlfile)\n",
    "    start = time.time()\n",
    "#   iteration=0\n",
    "    for chunk in pd.read_csv(csvfile,chunksize=1000):\n",
    "        chunk.to_sql(name='content',con=sqlconnection,if_exists=\"append\",index=False)\n",
    "    sqlsavetime = time.time()-start\n",
    "    return sqlsavetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time to query in the sqlite db\n",
    "# We are querying for the row where the timestamp is '2020-12-11 03:31:30'\n",
    "\n",
    "sqlstatement = \"SELECT * FROM content WHERE timestamp IS '2020-12-11 03:31:30'\"\n",
    "\n",
    "def calculate_sql_query_time(sqlfile,sqlstatement):\n",
    "    start = time.time()\n",
    "    sqlconnection = sqlite3.connect(sqlfile)\n",
    "    sqlcursor = sqlconnection.cursor()\n",
    "    sqlcursor.execute(sqlstatement)\n",
    "    sqlcursor.fetchall()\n",
    "    sqlquerytime = time.time()-start\n",
    "    return sqlquerytime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time to read the sql db\n",
    "\n",
    "query = \"SELECT * FROM content\"\n",
    "dfsql = None\n",
    "\n",
    "def calculate_sql_to_df_load_time(query,sqlfile,dfsql):\n",
    "    start = time.time()\n",
    "    sqlconnection = sqlite3.connect(sqlfile)\n",
    "    dfsql = pd.read_sql_query(query,sqlconnection)\n",
    "    sqlreadtime = time.time()-start\n",
    "    return sqlreadtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time to save the pickle file\n",
    "\n",
    "picklfile = \"hydraulic.pkl\"\n",
    "\n",
    "def calculate_df_to_pickle_time(picklfile,df):\n",
    "    start = time.time()\n",
    "    with open(picklfile,\"wb\") as f:\n",
    "        pickle.dump(df,f)\n",
    "    picklesavetime = time.time()-start\n",
    "    return picklesavetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time to load the pickle file content as dataframe in pandas\n",
    "\n",
    "dfpkl = None\n",
    "\n",
    "def calculate_pickl_to_df_load_time(picklfile,dfpkl):\n",
    "    start = time.time()\n",
    "    with open(picklfile,\"rb\") as f:\n",
    "        dfpkl = pickle.load(f)\n",
    "    picklereadtime = time.time()-start\n",
    "    return picklereadtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time to save the dataframe as hdf5 file\n",
    "# SKIPPED THIS PORTION WHEN CALCULATING THE TIME AT THE END\n",
    "\n",
    "hdfile = 'hydraulic.h5'\n",
    "\n",
    "def calculate_df_to_hdf5_file_time(hdfile,df):\n",
    "    start = time.time()\n",
    "    df.to_hdf(hdfile,key='df',mode='w')\n",
    "    hdfsavetime = time.time()-start\n",
    "    return dhfsavetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time to read the hdf5 file\n",
    "# ERROR IN READING THE FILE OF HDF5 FORMAT\n",
    "\n",
    "start = time.time()\n",
    "dfhdf = pd.read_hdf('hydraulic.h5',mode='r+')\n",
    "hdfreadtime = time.time()-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time to save the dataframe as a parquet file\n",
    "# Approach 1 - without using pytables\n",
    "\n",
    "parquet_file1 = 'df.parquet.gzip'\n",
    "compression_type = 'gzip'\n",
    "\n",
    "def calculate_df_to_parquet_save_time_approach1(parquet_file,compression_type,df):\n",
    "    start = time.time()\n",
    "    df.to_parquet(parquet_file,compression=compression_type)\n",
    "    parquetsavetime = time.time()-start\n",
    "    return parquetsavetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time to read the parquet file to the dataframe\n",
    "# Approach 1\n",
    "\n",
    "dfparquet1 = None\n",
    "\n",
    "def calculate_parquet_to_df_load_time_approach1(parquet_file,dfparquet):\n",
    "    start = time.time()\n",
    "    dfparquet = pd.read_parquet(parquet_file)\n",
    "    parquetloadtime = time.time()-start\n",
    "    return parquetloadtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time to save the parquet file to the dataframe\n",
    "# Approach 2 - using pytables\n",
    "\n",
    "parquet_file2 = 'df.parquet'\n",
    "\n",
    "def calculate_df_to_parquet_save_time_approach2(parquet_file,df):\n",
    "    start = time.time()\n",
    "    table = pa.Table.from_pandas(df)\n",
    "    pq.write_table(table,parquet_file)\n",
    "    parquetsavetime = time.time()-start\n",
    "    return parquetsavetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time to read the parquet file to the dataframe\n",
    "# Approach 2 - using pytables\n",
    "\n",
    "dfparquet2 = None\n",
    "\n",
    "def calculate_parquet_to_df_load_time_approach2(parquet_file,dfparquet):\n",
    "    start = time.time()\n",
    "    table = pq.read_table(parquet_file)\n",
    "    dfparquet = table.to_pandas()\n",
    "    parquetloadtime = time.time()-start\n",
    "    return parquetloadtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time to save cPickle file\n",
    "\n",
    "cpklfile = 'hydraulicc.pkl'\n",
    "\n",
    "def calculate_df_to_cpkl_save_time(cpklfile,df):\n",
    "    start = time.time()\n",
    "    with open(cpklfile,\"wb\") as f:\n",
    "        cPickle.dump(df,f)\n",
    "    cpicklesavetime = (time.time()-start)\n",
    "    return cpicklesavetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time to load cPickle file\n",
    "\n",
    "dfcpkl = None\n",
    "\n",
    "def calculate_cpkl_to_df_load_time(cpklfile,dfcpkl):\n",
    "    start = time.time()\n",
    "    with open(cpklfile,\"rb\") as f:\n",
    "        dfcpkl = cPickle.load(f)\n",
    "    cpickleloadtime = time.time()-start\n",
    "    return cpickleloadtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time to save dataframe as feather\n",
    "\n",
    "featherfile = 'hydraulic.feather'\n",
    "\n",
    "def calculate_df_to_feather_save_time(featherfile,df):\n",
    "    start = time.time()\n",
    "    feather.write_dataframe(df,featherfile)\n",
    "    feathersavetime = time.time()-start\n",
    "    return feathersavetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time to read the feather file\n",
    "\n",
    "dffeather = None\n",
    "\n",
    "def calculate_feather_to_df_load_time(featherfile,dffeather):\n",
    "    start = time.time()\n",
    "    dffeather = feather.read_dataframe(featherfile)\n",
    "    featherloadtime = time.time()-start\n",
    "    return featherloadtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time to query the dataframe\n",
    "# Say we are quering to find the row with timestamp 2020-12-11 03:31:30\n",
    "\n",
    "def calculate_time_to_query_df(df):\n",
    "    start = time.time()\n",
    "    df.loc[df['timestamp'] == \"2020-12-11 03:31:30\"]\n",
    "    dfquerytime = time.time()-start\n",
    "    return dfquerytime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV read times: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/quartic/quarticai/learn-projects/lenv/lib/python3.6/site-packages/pandas/core/generic.py:2130: UserWarning: The spaces in these column names will not be changed. In pandas versions < 0.14, spaces were converted to underscores.\n",
      "  dtype=dtype)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DF to SQL times: 1\n",
      "SQL Query times: 1\n",
      "SQL to DF times: 1\n",
      "DF to Pickle times: 1\n",
      "Pickle to DF times: 1\n",
      "DF to Parquet approach 1 times: 1\n",
      "Parquet to DF approach 1 times: 1\n",
      "DF to Parquet approach 2 times: 1\n",
      "Parquet to DF approach 2 times: 1\n",
      "DF to cPickle times: 1\n",
      "cPickle to DF times: 1\n",
      "DF to feather times: 1\n",
      "Feather to DF times: 1\n",
      "Query DF times: 1\n",
      "CSV read times: 2\n",
      "DF to SQL times: 2\n",
      "SQL Query times: 2\n",
      "SQL to DF times: 2\n",
      "DF to Pickle times: 2\n",
      "Pickle to DF times: 2\n",
      "DF to Parquet approach 1 times: 2\n",
      "Parquet to DF approach 1 times: 2\n",
      "DF to Parquet approach 2 times: 2\n",
      "Parquet to DF approach 2 times: 2\n",
      "DF to cPickle times: 2\n",
      "cPickle to DF times: 2\n",
      "DF to feather times: 2\n",
      "Feather to DF times: 2\n",
      "Query DF times: 2\n",
      "CSV read times: 3\n",
      "DF to SQL times: 3\n",
      "SQL Query times: 3\n",
      "SQL to DF times: 3\n",
      "DF to Pickle times: 3\n",
      "Pickle to DF times: 3\n",
      "DF to Parquet approach 1 times: 3\n",
      "Parquet to DF approach 1 times: 3\n",
      "DF to Parquet approach 2 times: 3\n",
      "Parquet to DF approach 2 times: 3\n",
      "DF to cPickle times: 3\n",
      "cPickle to DF times: 3\n",
      "DF to feather times: 3\n",
      "Feather to DF times: 3\n",
      "Query DF times: 3\n",
      "CSV read times: 4\n",
      "DF to SQL times: 4\n",
      "SQL Query times: 4\n"
     ]
    }
   ],
   "source": [
    "# Calculating average time calculations\n",
    "\n",
    "n = 100\n",
    "read_csv_times = []\n",
    "df_to_sql_times = []\n",
    "query_sql_times = []\n",
    "sql_to_df_times = []\n",
    "df_to_pkl_times = []\n",
    "pkl_to_df_times = []\n",
    "df_to_parquet_app1_times = []\n",
    "parquet_to_df_app1_times = []\n",
    "df_to_parquet_app2_times = []\n",
    "parquet_to_df_app2_times = []\n",
    "df_to_cpkl_times = []\n",
    "cpkl_to_df_times = []\n",
    "df_to_feather_times = []\n",
    "feather_to_df_times = []\n",
    "query_df_times = []\n",
    "\n",
    "for i in range(0,n):\n",
    "    read_csv_time = calculate_read_csv_time(csvfile)\n",
    "    read_csv_times.append(read_csv_time)\n",
    "    \n",
    "    print(\"CSV read times:\",i+1)\n",
    "    \n",
    "    df_to_sql_time = calculate_df_to_sql_save_time(csvfile,sqlfile)\n",
    "    df_to_sql_times.append(df_to_sql_time)\n",
    "    \n",
    "    print(\"DF to SQL times:\",i+1)\n",
    "    \n",
    "    query_sql_time = calculate_sql_query_time(sqlfile,sqlstatement)\n",
    "    query_sql_times.append(query_sql_time)\n",
    "    \n",
    "    print(\"SQL Query times:\",i+1)\n",
    "    \n",
    "    sql_to_df_time = calculate_sql_to_df_load_time(query,sqlfile,dfsql)\n",
    "    sql_to_df_times.append(sql_to_df_time)\n",
    "    \n",
    "    print(\"SQL to DF times:\",i+1)\n",
    "    \n",
    "    df_to_pkl_time = calculate_df_to_pickle_time(picklfile,df)\n",
    "    df_to_pkl_times.append(df_to_pkl_time)\n",
    "    \n",
    "    print(\"DF to Pickle times:\",i+1)\n",
    "    \n",
    "    pkl_to_df_time = calculate_pickl_to_df_load_time(picklfile,dfpkl)\n",
    "    pkl_to_df_times.append(pkl_to_df_time)\n",
    "    \n",
    "    print(\"Pickle to DF times:\",i+1)\n",
    "    \n",
    "    df_to_parquet_app1_time = calculate_df_to_parquet_save_time_approach1(parquet_file1,compression_type,df)\n",
    "    df_to_parquet_app1_times.append(df_to_parquet_app1_time)\n",
    "    \n",
    "    print(\"DF to Parquet approach 1 times:\",i+1)\n",
    "    \n",
    "    parquet_to_df_app1_time = calculate_parquet_to_df_load_time_approach1(parquet_file1,dfparquet1)\n",
    "    parquet_to_df_app1_times.append(parquet_to_df_app1_time)\n",
    "    \n",
    "    print(\"Parquet to DF approach 1 times:\",i+1)\n",
    "    \n",
    "    df_to_parquet_app2_time = calculate_df_to_parquet_save_time_approach2(parquet_file2,df)\n",
    "    df_to_parquet_app2_times.append(df_to_parquet_app2_time)\n",
    "    \n",
    "    print(\"DF to Parquet approach 2 times:\",i+1)\n",
    "    \n",
    "    parquet_to_df_app2_time = calculate_parquet_to_df_load_time_approach2(parquet_file2,dfparquet2)\n",
    "    parquet_to_df_app2_times.append(parquet_to_df_app2_time)\n",
    "    \n",
    "    print(\"Parquet to DF approach 2 times:\",i+1)\n",
    "    \n",
    "    df_to_cpkl_time = calculate_df_to_cpkl_save_time(cpklfile,df)\n",
    "    df_to_cpkl_times.append(df_to_cpkl_time)\n",
    "    \n",
    "    print(\"DF to cPickle times:\",i+1)\n",
    "    \n",
    "    cpkl_to_df_time = calculate_cpkl_to_df_load_time(cpklfile,dfcpkl)\n",
    "    cpkl_to_df_times.append(cpkl_to_df_time)\n",
    "    \n",
    "    print(\"cPickle to DF times:\",i+1)\n",
    "    \n",
    "    df_to_feather_time = calculate_df_to_feather_save_time(featherfile,df)\n",
    "    df_to_feather_times.append(df_to_feather_time)\n",
    "    \n",
    "    print(\"DF to feather times:\",i+1)\n",
    "    \n",
    "    feather_to_df_time = calculate_feather_to_df_load_time(featherfile,dffeather)\n",
    "    feather_to_df_times.append(feather_to_df_time)\n",
    "    \n",
    "    print(\"Feather to DF times:\",i+1)\n",
    "    \n",
    "    query_df_time = calculate_time_to_query_df(df)\n",
    "    query_df_times.append(query_df_time)\n",
    "    \n",
    "    print(\"Query DF times:\",i+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hashing the pickle\n",
    "import hmac, hashlib\n",
    "pickled_data = pickle.dumps(df)\n",
    "digest = hmac.new('key',pickled_data,hashlib.sha1).hexdigest()\n",
    "header = '%s' % (digest)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lenv",
   "language": "python",
   "name": "lenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
